{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aimalz/galevo23-tutorials/blob/main/week-4/Malz_KITPCCA_photo-z_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center>Photo-$z$s and uncertainties thereof: A tutorial</center>\n",
        "#### <center> **[Alex I. Malz](https://github.com/aimalz)**</center>\n",
        "#### <center> LINCC Frameworks @ Carnegie Mellon University </center>\n",
        "#### <center>  </center>\n",
        "#### <center> [KITP-CCA Workshop: Data Driven Galaxy Evolution](https://datadrivengalaxyevolution.github.io/#ccaweek) </center>"
      ],
      "metadata": {
        "id": "s0TLpSfV1iug"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJJCNEYMp64R"
      },
      "source": [
        "This tutorial was adapted from [a problem set](https://github.com/LSSTC-DSFP/LSSTC-DSFP-Sessions/blob/main/Sessions/Session16/Day4/expdes-photoz.ipynb) at the [LSSTC Data Science Fellowship Program](https://astrodatascience.org/), which was in turn adapted from a data challenge at a workshop;\n",
        "if you use anything from here in a publication, cite [the repo it originally came from](https://github.com/aimalz/qtc2021) and acknowledge \"[From Quarks to Cosmos with AI](https://events.mcs.cmu.edu/qtc2021/), a conference supported by the NSF AI Institute: Physics of the Future, NSF PHY-2020295.\" \n",
        "\n",
        "## Overview\n",
        "\n",
        "As we look ahead to inferring the physics of galaxy evolution from deep catalogs lacking spectroscopy, we will very likely need to rely on photometric redshifts (photo-$z$s) for large-scale analyses.\n",
        "This tutorial assumes you know the basic idea and have some familiarity with how redshift estimates factor into the kinds of analyses you want to do.\n",
        "My goal is to impart an appreciation for the nuances of photo-$z$s we have to anticipate for LSST data to help us all get the most out of them.\n",
        "\n",
        "This notebook provides some choices of photo-$z$ estimation models, options for data and priors upon which to test the models, and a variety of metrics to evaluate the estimates, an end-to-end pipeline for \n",
        "Your objective is to uncover the unstated assumptions, probe the limiting cases, and overall figure out how to break the experiment, thereby identifying what you need to do to ensure photo-$z$ data products suit your needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O19PzXur-GF"
      },
      "source": [
        "## Motivation\n",
        "\n",
        "More and more data-driven methods yield uncertainties $\\hat{p}(y | x_{i})$ of target parameters $y$ given observed random variables $x_{i}$, rather than just point estimates $\\hat{y}_{i}$.\n",
        "Though rarely framed as such, these uncertainties are _posteriors_, and there's more than meets the eye to what lies on the righthand side of the conditional.\n",
        "\n",
        "Really, an estimated posterior should be writen as $p(y | x_{i}, \\pi, M)$ for prior $\\pi$ (which is training data $\\{y_{n}, x_{n}\\}_{N}$ for a machine learning approach and takes other forms for physics-informed models) and algorithm (model) $M$.\n",
        "Though the dependence on prior information is straightforward, the dependence on the algorithm, meaning the estimation model and its implementation, is subtle.\n",
        "However, if it were not there, then every model with the same prior information would yield identical estimated uncertainties, which is not observed.\n",
        "While the $\\pi$ (e.g. training set $\\{y_{n}, x_{n}\\}_{N}$) is equivalent to an _explicit prior_, the algorithm $M$ must be considered an _implicit prior_, in that we don't know how to write down how it projects onto the space of data.\n",
        "\n",
        "Another way of looking at estimated posteriors is in terms of the type of uncertainty encompassed by each term on the righthand side of the conditional.\n",
        "For a noisy measurement or a stochastic generative process, the random variable $x_{i} \\sim p(x | y_{i})$ represents the _aleatoric uncertainty_, the uncertainty inherent to the data.\n",
        "However, the training set $\\{y_{n}, x_{n}\\}_{N}$ and implemented etimation model $M$ could potentially be improved to yield a better estimate and thus constitute sources of _epistemic uncertainty_, the uncertainty due to an imperfect model.\n",
        "In physics, we want to learn the aleatoric uncertainty $p(x | y_{i})$, which we can't get from the estimated posteriors $p(y | x_{i}, \\{y_{n}, x_{n}\\}_{N}, M)$ in hand without knowing $p(y | \\{y_{n}, x_{n}\\}_{N}, M)$.\n",
        "\n",
        "Meanwhile, assessments of the performance of estimated $p(y | x_{i}, \\{y_{n}, x_{n}\\}_{N}, M)$ are almost always made by comparison to known $y_{i}$, leaving unanswered the question of how well the estimator approximates $p(y | x_{i})$.\n",
        "Why?\n",
        "The problem is that $p(y | x_{i})$ is not necessarily known, certainly not for observed $y_{i}$ measured in nature, but also generally for simulated $y_{i}$, at least in astrophysics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW-T6Zq70MwL"
      },
      "source": [
        "## Context: Photometric redshifts\n",
        "\n",
        "[Photo-$z$s](https://en.wikipedia.org/wiki/Photometric_redshift) provide an excellent testbed for addressing these issues but require some introduction.\n",
        "There isn't a definitive primer, but here are several overviews that are informative, if a bit dry.\n",
        "- [basic intro from Rubin Observatory](https://www.lsst.org/science/dark-energy/photometric-redshift)\n",
        "- [old overview covering classic concepts](https://ned.ipac.caltech.edu/level5/Glossary/Essay_photredshifts.html)\n",
        "- [recent review of estimation methods](https://arxiv.org/abs/1805.12574)\n",
        "If these don't answer your questions, Chapter 0 of [my thesis](https://zenodo.org/record/3973536) might, or, better yet, [the slides from my defense](https://github.com/aimalz/ship-of-theses/tree/master/presentation) make for a better tl;dr.\n",
        "\n",
        "Photo-$z$s are an ideal system to study because they are simple enough to obtain $p(y | x_{i})$ along the way to generating a sample of $(y_{i}, x_{i})$ pairs.\n",
        "Here, the target variable $y \\to z$ is redshift, a scalar, and the data $x \\to \\vec{d} = (u, g, r, i, z, y)$, or some trivial function thereof, is a vector of length $<10$ observed [photometric magnitudes](https://en.wikipedia.org/wiki/Magnitude_(astronomy)) of galaxies through [broadband optical filters](https://en.wikipedia.org/wiki/Photometric_system) (which I somewhat arbitrarily choose to be those of [the Vera C. Rubin Observatory](https://www.lsst.org/)).\n",
        "Because of the extremely low dimensionality of the problem, we can forward model not just individual pairs $(z_{i}, \\vec{d}_{i})$ but the entire joint probability space $p(z, \\vec{d})$, thereby obtaining $p(y | x_{i})$ for every $(z_{i}, \\vec{d}_{i})$.\n",
        "That doesn't mean it's trivial to do so -- most of this tutorial concerns that forward-modeling procedure -- but it is possible.\n",
        "\n",
        "Another reason photo-$z$s are apt is that comprehensive uncertainty quantification for galaxy redshifts in the absence of spectroscopy are crucial for the Legacy Survey of Space and Time (LSST), an upcoming photometric survey on the Rubin Observatory.\n",
        "This tutorial makes use of two pieces of software under public development, [RAIL](https://github.com/LSSTDESC/RAIL) and [qp](https://github.com/LSSTDESC/qp), whose functionality will be introduced where relevant, along with other code dependencies.\n",
        "Participants need not use either in their responses to the challenge questions, but the development team welcomes feedback from potential users and new developers, no collaboration membership required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIOag5iVqZPW"
      },
      "source": [
        "# Tutorial and challenge prompts\n",
        "\n",
        "Finally, without more ado, we set the stage with a tutorial that:\n",
        "1. creates realistically complex mock photo-$z$ posteriors, redshifts, and photometry to define training and test sets;\n",
        "2. estimates photo-$z$ posteriors of the test set given the training set;\n",
        "3. quantifies how closely the estimated posteriors approximate the true ones.\n",
        "\n",
        "The three-pronged structure of this tutorial is inspired by that of [RAIL](https://github.com/LSSTDESC/RAIL), which has subpackages corresponding to each of the enumerated parts of the tutorial: `rail.creation`, `rail.estimation`, and `rail.evaluation`.\n",
        "You can explore them in their native habitat [in this separate Colab notebook](https://colab.research.google.com/drive/1YCWRdbNfslAhuxYaxgp_3M9Q37jBPLd6?usp=sharing) (also in the tutorial repository), but I took out the pieces here to make it easier to see what's going on, even though RAIL would be far more appropriate for an at-scale pipeline. \n",
        "\n",
        "The challenge comes from building on the tutorial content to conduct a self-guided investigation of the open questions.\n",
        "To get the most out of this opportunity, think of this tutorial as a lab manual that presents an experimental procedure to answer each question and includes an example of a possible solution, then invites participants to \"riff\" on those solutions to devise and implement their own.\n",
        "Some opportunities for such investigations can be found under the \"Challenge\" headings throughout the tutorial, but feel free to make your own!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JituVGNVZ7bg"
      },
      "source": [
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2llNhgut6jM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn scipy==1.10.0 corner\n",
        "!pip install cde-diagnostics cdetools FlexCode xgboost==0.90\n",
        "!pip install pz-rail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNhx0u1vszZN"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHM0pdtvLdG2"
      },
      "outputs": [],
      "source": [
        "import cdetools\n",
        "import flexcode\n",
        "import pzflow\n",
        "import qp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZs_a1wy0EbC"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpl_patches\n",
        "mpl.rc('text', usetex=False)\n",
        "\n",
        "import corner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9imLKSxZ_F1"
      },
      "source": [
        "## 1. Generating mock data, including true photo-$z$ posteriors\n",
        "\n",
        "This is the most complex of the parts of the tutorial, which should leave readers with a sense of why this kind of experiment has not been done for higher-dimensional data.\n",
        "There are three main steps:\n",
        "1. Prepare and explore data to use as the basis for a realistically complex generative model.\n",
        "2. Emulate a few realistically complex $p(z, \\vec{d})$ probability spaces using input data.\n",
        "3. Sample $p(z, \\vec{d})$ and $p'(z, \\vec{d})$ to produce photometric data sets to use as explicit prior information and on which to estimate redshift posteriors, with known $z$, $\\vec{d}$, and $p(z | \\vec{d})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pexhHnZsoNq"
      },
      "source": [
        "### Data\n",
        "\n",
        "Though we're going to forward-model mock data to experiment on, we want it to be _realistically complex_, meaning it shares the physical degeneracies and systematic errors that would be present in a real data set.\n",
        "The [Happy/Teddy data sets](https://github.com/COINtoolbox/photoz_catalogues) (see [Beck, et al 2017](https://arxiv.org/abs/1701.08748) for full release notes) are curated subsamples of the [Sloan Digital Sky Survey (SDSS) Data Release (DR) 12](https://www.sdss.org/dr12/), a spectroscopic survey with high-fidelity redshift measurements, and were created by the [Cosmostatistics Initiative (COIN)](https://cosmostatistics-initiative.org/).\n",
        "The data sets are defined to emulate the kinds of differences in observational properties of galaxies with measured spectroscopic redshifts and those for which only photometry is available, and they were created with the goal of determining the impact of imbalance between training, validation, and test sets for photo-$z$ point estimation.\n",
        "They are thus an appropriate starting point for creating a realistically complex model of the joint probability space $p(z, \\vec{d})$.\n",
        "\n",
        "Of course there are plenty of other potential data sets available, including those that are simulated, which may be advantageous for the potential to estimate other galaxy properties such as stellar mass and star formation rate that are known in a simulation but not directly measurable with real data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/COINtoolbox/photoz_catalogues.git"
      ],
      "metadata": {
        "id": "SK9BYiudQnQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71MdMCuXn16X"
      },
      "outputs": [],
      "source": [
        "happy_path = './photoz_catalogues/Happy/happy_'\n",
        "header = pd.read_csv(happy_path+'A', delim_whitespace=True, nrows=0).columns[1:]\n",
        "teddy_path = './photoz_catalogues/Teddy/teddy_'\n",
        "\n",
        "happy, teddy = {}, {}\n",
        "for lett in ['A', 'B', 'C', 'D']:\n",
        "    happy[lett] = pd.read_csv(happy_path+lett, delim_whitespace=True, header=None, skiprows=1, names=header)\n",
        "    happy[lett] = happy[lett].rename(columns={'z_spec':'redshift', 'mag_r': 'r'})[['redshift', 'r', 'u-g', 'g-r', 'r-i', 'i-z']]\n",
        "#     happy[lett]\n",
        "    teddy[lett] = pd.read_csv(teddy_path+lett, delim_whitespace=True, header=None, skiprows=7, names=header)\n",
        "    teddy[lett] = teddy[lett].rename(columns={'z_spec':'redshift', 'mag_r': 'r'})[['redshift', 'r', 'u-g', 'g-r', 'r-i', 'i-z']]\n",
        "#     teddy[lett]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TV_VxG8CtoDW"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
        "for j, col in enumerate(['redshift', 'r']):\n",
        "    for i, lett in enumerate(['A', 'B', 'C', 'D']):\n",
        "        ax[j][0].hist(happy[lett][col], alpha=0.25, bins=100, density=False, label='happy_'+lett)\n",
        "        ax[j][1].hist(teddy[lett][col], alpha=0.25, bins=100, density=False, label='teddy_'+lett)\n",
        "        ax[j][0].set_xlabel(col)\n",
        "        ax[j][0].legend()\n",
        "        ax[j][1].set_xlabel(col)\n",
        "        ax[j][1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRQblNfe0cCQ"
      },
      "source": [
        "For convenience later on, we can safely cut off the tiny fraction of outliers in redshift and $r$-band magnitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WutxjLl4YvLG"
      },
      "outputs": [],
      "source": [
        "z_min, z_max = 0., 1.5\n",
        "r_min, r_max = 10., 25."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9YMBF_NsekS"
      },
      "outputs": [],
      "source": [
        "colorcycle = 'rbgcmy'\n",
        "fig = corner.corner(happy['A'][['u-g', 'g-r', 'r-i', 'i-z']], color='k', alpha=0.25)\n",
        "for i, lett in enumerate(['B', 'C', 'D']):\n",
        "    corner.corner(happy[lett][['u-g', 'g-r', 'r-i', 'i-z']], fig=fig, color=colorcycle[i], alpha=0.25)\n",
        "    \n",
        "fig = corner.corner(teddy['A'][['u-g', 'g-r', 'r-i', 'i-z']], color='k', alpha=0.25)\n",
        "for i, lett in enumerate(['B', 'C', 'D']):\n",
        "    corner.corner(teddy[lett][['u-g', 'g-r', 'r-i', 'i-z']], fig=fig, color=colorcycle[i], alpha=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuzPDc5WsrUK"
      },
      "source": [
        "### Emulating $p(z, \\vec{d})$ from input data\n",
        "\n",
        "[`pzflow`](https://github.com/jfcrenshaw/pzflow) is a package for making normalizing flows from sets of redshifts and photometry in order to estimate or otherwise model photo-$z$ posteriors.\n",
        "We'll use it to make a normalizing flow that will serve as the model for $p(z, \\vec{d})$.\n",
        "\n",
        "_This content is adapted from pzflow's [demo](https://github.com/jfcrenshaw/pzflow/blob/main/docs/tutorials/intro.ipynb), written by John Franklin Crenshaw (UW)._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yW3ONw4q9pw"
      },
      "outputs": [],
      "source": [
        "from pzflow import Flow\n",
        "from pzflow.bijectors import Chain, ColorTransform, InvSoftplus, StandardScaler, RollingSplineCoupling, ShiftBounds\n",
        "from pzflow.examples import get_galaxy_data\n",
        "from pzflow.distributions import Uniform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFJ8Lwxz0cCS"
      },
      "source": [
        "Let's start with a demonstration of how pzflow makes a model of $p(z, \\vec{d})$ from its own demo data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV1CUpkA0cCT"
      },
      "outputs": [],
      "source": [
        "data = get_galaxy_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxmVog1_kTqu"
      },
      "outputs": [],
      "source": [
        "data = get_galaxy_data()\n",
        "\n",
        "# restrict to Happy/Teddy range for coverage in demo\n",
        "data = data[(data['redshift'] > z_min) & (data['redshift'] < z_max) & (data['r'] > r_min) & (data['r'] < r_max)]\n",
        "\n",
        "# normalize\n",
        "data = data\n",
        "\n",
        "# use fewer bands to be able to compare with Happy/Teddy\n",
        "data = data[['redshift', 'u', 'g', 'r', 'i', 'z']]\n",
        "\n",
        "# convert magnitudes to a reference magnitude and colors\n",
        "data['u-g'] = data['u'] - data['g']\n",
        "data['g-r'] = data['g'] - data['r']\n",
        "data['r-i'] = data['r'] - data['i']\n",
        "data['i-z'] = data['i'] - data['z']\n",
        "\n",
        "# save the new set\n",
        "data = data[['redshift', 'r', 'u-g', 'g-r', 'r-i', 'i-z']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corner.corner(data)"
      ],
      "metadata": {
        "id": "W-qzrL_bRMDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KdppeUD5t7l"
      },
      "outputs": [],
      "source": [
        "# set the inverse softplus parameters, estimated\n",
        "# to ensure that sampled redshifts are positive\n",
        "column_idx = 0\n",
        "\n",
        "n_epoch = 30\n",
        "\n",
        "mins = [z_min, r_min, -1, -1, -1, -1]\n",
        "maxs = [z_max, r_max, 5, 5, 5, 5]\n",
        "\n",
        "# construct our bijector\n",
        "# by chaining all these layers\n",
        "bijector = Chain(\n",
        "    ShiftBounds(mins, maxs, B=5),\n",
        "    RollingSplineCoupling(nlayers=6)\n",
        ")\n",
        "\n",
        "\n",
        "flow = Flow(data.columns, bijector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIk6pJim0cCU"
      },
      "source": [
        "This step may be slow (~minutes) if you don't have a GPU runtime enabled (or if jax can't find the Colab GPU?)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWxpXjO10cCV"
      },
      "outputs": [],
      "source": [
        "losses = flow.train(data, epochs=n_epoch, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2d6air_0cCV"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEZM40DR0cCW"
      },
      "outputs": [],
      "source": [
        "flow.save('default_flow.pkl')\n",
        "flow = Flow(file='default_flow.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUcXXdjo0cCW"
      },
      "source": [
        "We need to choose a redshift grid upon which to evaluate the posterior PDFs.\n",
        "(Different engines for creating mock data may have different parameterizations that don't require this specification.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNl-Q4YqXaxc"
      },
      "outputs": [],
      "source": [
        "granularity = 100\n",
        "grid = np.linspace(z_min, z_max, granularity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alnb-qN40cCX"
      },
      "source": [
        "Let's check one of them before making more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS_Zs9Vqbs3x"
      },
      "outputs": [],
      "source": [
        "chosen = 999\n",
        "\n",
        "galaxy = data.iloc[[chosen]]\n",
        "pdf = flow.posterior(galaxy, column=\"redshift\", grid=grid)\n",
        "\n",
        "plt.plot(grid, pdf[0], label='Posterior')\n",
        "plt.axvline(galaxy['redshift'].values[0], 0, 1, c='C3', label='True redshift')\n",
        "plt.legend()\n",
        "plt.xlabel(\"redshift\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t6TnQ5e0cCY"
      },
      "source": [
        "Sample from the model of $p(z, data)$.\n",
        "(If you run out of memory, take fewer samples.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGrTo5J4TKuQ"
      },
      "outputs": [],
      "source": [
        "samples = flow.sample(10000, conditions=data[:5000], seed=0)\n",
        "plt.hist(data['redshift'], range=(0, 2.5), bins=40, histtype='step', label='data', density=True)\n",
        "plt.hist(samples['redshift'], range=(0, 2.5), bins=40, histtype='step', label='samples', density=True)\n",
        "plt.xlabel('z')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwGykJnbmqys"
      },
      "outputs": [],
      "source": [
        "z = samples['redshift']\n",
        "z.to_csv('test_set_redshifts.csv')\n",
        "\n",
        "phot = samples[['r', 'u-g', 'g-r', 'r-i', 'i-z']]\n",
        "phot.to_csv('test_set_photometry.csv')\n",
        "\n",
        "posteriors = flow.posterior(samples, column=\"redshift\", grid=grid)\n",
        "with open('test_set_posteriors.csv', 'wb') as fn:\n",
        "    jnp.save(fn, posteriors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyecUsXrJJ7R"
      },
      "source": [
        "We can do this procedure for all the Happy/Teddy samples so we can experiment with them later.\n",
        "It's slow without using GPU, but it only needs to be done once.\n",
        "(The repo also contains a few of these trained models to play with.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_data = {'teddy': teddy}#'happy': happy\n",
        "# not sure why Happy data sets won't train on colab now?\n",
        "n_out = 1000"
      ],
      "metadata": {
        "id": "D4EQPfAgGIf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvYgIMnWuq9b"
      },
      "outputs": [],
      "source": [
        "for name, dat in full_data.items():\n",
        "    for lett in ['A', 'B', 'C':]#, 'D']:\n",
        "      # also not sure why teddyD won't train anymore. . .\n",
        "        print(name+lett)\n",
        "        print(dat[lett].columns)\n",
        "        print(len(dat[lett]))\n",
        "        \n",
        "        flow = Flow(dat[lett].columns, bijector)\n",
        "\n",
        "        losses = flow.train(dat[lett], epochs=n_epoch, verbose=True)\n",
        "        flow.save(name+lett+'flow.pkl')\n",
        "        print('done with '+name+lett)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU-efFb50cCa"
      },
      "outputs": [],
      "source": [
        "for name, dat in full_data.items():\n",
        "    for lett in ['A', 'B', 'C']:#, 'D']:     \n",
        "        print(name+lett)\n",
        "        flow = Flow(file=name+lett+'flow.pkl')\n",
        "\n",
        "        samples = flow.sample(10000, conditions=dat[lett].sample(n_out), seed=0)\n",
        "\n",
        "        z = samples['redshift']\n",
        "        z.to_csv(name+lett+'redshifts.csv', index=False)\n",
        "\n",
        "        phot = samples[['r', 'u-g', 'g-r', 'r-i', 'i-z']]\n",
        "        phot.to_csv(name+lett+'photometry.csv', index=False)\n",
        "\n",
        "        posteriors = flow.posterior(samples, column=\"redshift\", grid=grid)\n",
        "        with open(name+lett+'posteriors.csv', 'wb') as fn:\n",
        "            jnp.save(fn, posteriors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHVvqswx0cCa"
      },
      "source": [
        "### Challenge 1\n",
        "\n",
        "Try to \"degrade\" the drawn data to introduce degeneracies, inconsistencies, and nonrepresentativity that could trip up an estimator with or without being missed by a metric.\n",
        "Show what that photometry looks like and save it to feed into estimators later in the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvB8x8sE0cCa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xpQw49TaR03"
      },
      "source": [
        "## 2. Estimating photo-z posterior PDFs\n",
        "\n",
        "There are many estimators of photo-$z$ posteriors, and many of those are compared to one another in [Schmidt & Malz, et al. 2020](https://arxiv.org/abs/2001.03621).\n",
        "The outcomes of that experiment inspired the design of the [Redshift Assessment Infrastructure Layers (RAIL](https://github.com/LSSTDESC/RAIL) codebase, an open-source toolkit that provides a unified API for many photo-$z$ estimators and instructions for wrapping additional ones to be included.\n",
        "If you need to obtain photo-$z$ PDFs at scale, the [Golden Spike](https://github.com/LSSTDESC/RAIL/tree/main/examples/goldenspike) end-to-end demo is a great starting point.\n",
        "(This repository also now has a Colab-ified version of the Golden Spike for convenience.)\n",
        "\n",
        "For many galaxy evolution applications, a template-based estimator would be more appropriate, as it could output a posterior $p(z, \\alpha | data)$, where $\\alpha$ could be parameters like SFR or stellar mass.\n",
        "There are a couple reasons why I'm omitting them from this tutorial:\n",
        "1. Storage of the multivariate posteriors $\\{p(z, \\alpha | data)\\}$ is nontrivial; there is no recommended compression scheme so no infrastructure for it in `qp` yet.\n",
        "2. Template-based estimators tend to have complicated setups due to needing data files for the physical information, i.e. templates, priors, etc.; the paths for these are a bit difficult to wrangle on Colab and I didn't leave enough time to prepare for it before this tutorial. XD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZsmK6L8v9OO"
      },
      "source": [
        "### Estimating photo-$z$ posterior PDFs with FlexCode\n",
        "\n",
        "\n",
        "Of the tested estimators in the aforementioned experiment, including ML and non-ML methods, the most promising was [`FlexCode`](https://github.com/tpospisi/FlexCode) ([Izbicki & Lee, 2017](https://arxiv.org/abs/1704.08095)), which also happens to be one of the easiest to install and apply as a standalone code, so we'll use it as an example of an estimator of photo-$z$ posteriors.\n",
        "\n",
        "A demonstration of `FlexCode` in the context of photo-$z$s can be found in [Dalmasso, et al 2019](https://arxiv.org/abs/1908.11523), with demos in `R` published on [GitHub](https://github.com/Mr8ND/cdetools_applications).\n",
        "We'll demonstrate it on the pzflow-based samples generated from the Happy/Teddy data sets.\n",
        "\n",
        "_This content is adapted from FlexCode's [Teddy tutorial](https://github.com/tpospisi/FlexCode/blob/master/tutorial/Flexcode-tutorial-teddy.ipynb), written by Nic Dalmasso (CMU)._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGZ8i_opZiFY"
      },
      "outputs": [],
      "source": [
        "n_grid = granularity\n",
        "\n",
        "# Select regression method\n",
        "from flexcode.regression_models import NN\n",
        "\n",
        "# Parameters\n",
        "basis_system = \"cosine\"  # Basis system\n",
        "max_basis = 31           # Maximum number of basis. If the model is not tuned,\n",
        "                         # max_basis is set as number of basis\n",
        "\n",
        "n_estimators = 100\n",
        "criterion = 'mse'\n",
        "max_depth = 5\n",
        "    \n",
        "# Regression Parameters \n",
        "# If a list is passed for any parameter automatic 5-fold CV is used to\n",
        "# determine the best parameter combination.\n",
        "params = {\"k\": 20}#[5, 10, 15, 20]}       # A dictionary with method-specific regression parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udbXVbiDZwie"
      },
      "source": [
        "Let's try first with a representative training/validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb0yIUIjXSJm"
      },
      "outputs": [],
      "source": [
        "x_orig = pd.read_csv('test_set_photometry.csv')[['r', 'u-g', 'g-r', 'r-i', 'i-z']].to_numpy()\n",
        "y_orig = pd.read_csv('test_set_redshifts.csv')[['redshift']].to_numpy()\n",
        "posteriors_orig = pd.DataFrame(np.load('test_set_posteriors.csv')).to_numpy()\n",
        "\n",
        "# n_samp = 10000\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test, posteriors_train, posteriors_test = train_test_split(x_orig, y_orig, posteriors_orig, \n",
        "                                                                                       train_size=2000, random_state=42)\n",
        "x_train, x_validation, y_train, y_validation, posteriors_train, posteriors_validation = train_test_split(x_train, y_train, posteriors_train, \n",
        "                                                                                                         train_size=1000, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo6J8wY7pb1U"
      },
      "outputs": [],
      "source": [
        "# Parameterize model\n",
        "model = flexcode.FlexCodeModel(NN, max_basis, basis_system, regression_params=params)\n",
        "\n",
        "# Fit model - this will also choose the optimal number of neighbors `k`\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Tune model - Select the best number of basis\n",
        "model.tune(x_validation, y_validation)\n",
        "\n",
        "# Predict new densities on grid\n",
        "cde_test, y_grid = model.predict(x_test, n_grid=n_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXNaeqy80cCc"
      },
      "source": [
        "We can examine one of the photo-$z$ posteriors estimated with the perfectly representative training/validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ8plrbNCoLn"
      },
      "outputs": [],
      "source": [
        "chosen = 9\n",
        "\n",
        "plt.plot(y_grid, cde_test[chosen], label='Estimated posterior')\n",
        "plt.plot(grid, posteriors_test[chosen], label='True posterior')\n",
        "plt.axvline(y_test[chosen], 0, 1, c='C3', label='True redshift')\n",
        "plt.legend()\n",
        "plt.xlabel(\"redshift\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAF62sfOawmd"
      },
      "source": [
        "It looks pretty good!\n",
        "Now let's try training and validating with some Happy/Teddy data but estimating posteriors on the test set from the pzflow demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWljb73yRRlT"
      },
      "outputs": [],
      "source": [
        "y_train = pd.read_csv('teddyAredshifts.csv')['redshift'].to_numpy()\n",
        "x_train = pd.read_csv('teddyAphotometry.csv')[['r', 'u-g', 'g-r', 'r-i', 'i-z']].to_numpy()\n",
        "\n",
        "y_validation = pd.read_csv('teddyBredshifts.csv')['redshift'].to_numpy()\n",
        "x_validation = pd.read_csv('teddyBphotometry.csv')[['r', 'u-g', 'g-r', 'r-i', 'i-z']].to_numpy()\n",
        "\n",
        "# similar for TeddyC?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmNhNgiIZp12"
      },
      "outputs": [],
      "source": [
        "# Parameterize model\n",
        "model = flexcode.FlexCodeModel(NN, max_basis, basis_system, regression_params=params)\n",
        "\n",
        "# Fit model - this will also choose the optimal number of neighbors `k`\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# # # Tune model - Select the best number of basis\n",
        "model.tune(x_validation, y_validation)\n",
        "\n",
        "# # Predict new densities on grid\n",
        "cde_test_bias, y_grid_bias = model.predict(x_test, n_grid=n_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwnYgZL6aWUP"
      },
      "outputs": [],
      "source": [
        "plt.plot(y_grid_bias, cde_test_bias[chosen], label='Estimated posterior (biased training set)')\n",
        "plt.plot(y_grid, cde_test[chosen], label='Estimated posterior (unbiased training set)')\n",
        "plt.plot(grid, posteriors_test[chosen], label='True posterior')\n",
        "plt.axvline(y_test[chosen], 0, 1, c='C3', label='True redshift')\n",
        "plt.legend()\n",
        "plt.xlabel(\"redshift\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS6e_O5z0cCf"
      },
      "source": [
        "As expected, the biased training and validation sets worsen the estimated posterior PDFs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r3n6nO80cCf"
      },
      "source": [
        "### Challenge 2a\n",
        "\n",
        "Try retraining `FlexCode` and estimating photo-$z$ posteriors on different combinations of the data sets, as well as different hyperparameters of the estimator, so you have some options to compare.\n",
        "Hypothesize which test cases will perform well and which will not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOazR3ZT0cCf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AfY78ne0cCf"
      },
      "source": [
        "### Challenge 2b\n",
        "\n",
        "`trainZ` was the \"winner\" of the DESC PZ DC1 experiment by traditional metrics.\n",
        "The algorithm is simple: $\\hat{p}(z | \\vec{d}_{i}) = p(z | \\{d_{train}\\})$, i.e.  each test set galaxy's estimated photo-$z$ posterior is the redshift distribution of the training set.\n",
        "Try implementing it here and saving the outputs for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrbFvclE0cCf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B30Mv427j5td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenge 2c\n",
        "\n",
        "`pzflow` is itself an estimator, even though we used it as a forward model of mock data here.\n",
        "Try using one of the models trained earlier as an estimator for data generated by another model to see how it performs in the absence of representativity."
      ],
      "metadata": {
        "id": "fyh6M3qEj51R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4JswNJ2mkNOf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg4SzDIYiTGf"
      },
      "source": [
        "## 3. Evaluating the performance of estimated photo-$z$ posterior PDFs\n",
        "\n",
        "Once we have estimated photo-$z$ posterior PDFs, we need a way to determine if they're actually any good.\n",
        "Since the tutorial has only one method but multiple training/validation sets, that's all we can compare for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fQCmfJE0cCg"
      },
      "source": [
        "### Photo-$z$ point estimates (summary statistics)\n",
        "\n",
        "A lot of use cases of photo-$z$ data products accept only a point estimate $\\hat{z}$, so many science-motivated metrics will be defined in terms of the ultimate output of an analysis that ingests such point estimates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsqLV2Pv0cCh"
      },
      "source": [
        "###Challenge 3a\n",
        "\n",
        "Implement a few scalar summary statistics from the PDFs, e.g. the mean, median, and mode.\n",
        "Derive those point estimates from the different sets of estimated photo-$z$ posteriors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj8IUMIa0cCh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMurG0Pk0cCi"
      },
      "source": [
        "### Metrics of photo-$z$ point estimates and reference redshifts\n",
        "\n",
        "Traditional metrics of photo-$z$ data products compare the estimates $\\{\\hat{z}_{i}\\}$ to reference values, either true redshifts from a simulation or spectroscopic redshifts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jSOjNf60cCh"
      },
      "source": [
        "###Challenge 3b\n",
        "\n",
        "Make a scatterplot of the point estimates versus the true redshifts.\n",
        "Include marginal histograms of the distributions of true and point-estimate redshifts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj_-rnwt0cCi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmR8iJII0cCi"
      },
      "source": [
        "### Challenge 3c\n",
        "\n",
        "The _bias_ is usually defined as $\\frac{\\hat{z} - z_{true}}{1+z_{true}}$, although [Graham+18](http://stacks.iop.org/1538-3881/155/i=1/a=1) defines a \"robust\" version as $\\frac{\\hat{z} - z_{true}}{1+\\hat{z}}$.\n",
        "Implement these, evaluate them on your test cases, and plot the results as histograms or averages in redshift bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNLf29eo0cCi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8PH_fAw0cCi"
      },
      "source": [
        "### Challenge 3d\n",
        "\n",
        "The _scatter_ is usually defined as $\\sigma_{z} = \\frac{(\\hat{z} - z_{true})^{2}}{1+z_{true}}$.\n",
        "Implement it, evaluate it on your test cases, and plot the results as histograms or averages in redshift bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw-qCpJX0cCj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pq5uDBh0cCk"
      },
      "source": [
        "### Challenge 3e\n",
        "The catastrophic outlier rate is defined as the fraction of galaxies for which $|\\hat{z} - z_{true}| > 3 * \\sigma_{z}$.\n",
        "Implement it, calculate it on your test cases, and plot the results as histograms or averages in redshift bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi7AdQIm0cCk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYd_evyhw3pC"
      },
      "source": [
        "### Metrics of estimated photo-$z$ posteriors and reference redshifts\n",
        "\n",
        "First, let's try out a couple metrics of estimated photo-$z$ posteriors that do not require knowledge of the true photo-$z$ posteriors.\n",
        "There's additional functionality for the case of having true redshifts but not true posteriors in [cdetools](https://github.com/tpospisi/cdetools) and [cde-diagnostics](https://github.com/zhao-david/CDE-diagnostics), but this should give a general idea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L53A-iBubIvZ"
      },
      "outputs": [],
      "source": [
        "from cdetools import cde_loss, cdf_coverage, hpd_coverage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sVDo_DLgGit"
      },
      "source": [
        "The Probability Integral Transform (PIT) is defined as \n",
        "\\begin{equation}\n",
        "PIT = \\int_{-\\infty}^{z_{true}} p(z | \\vec{d}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi) dz .\n",
        "\\end{equation}\n",
        "A histogram of PIT values is commonly used to assess how consistent a population of photo-$z$ PDFs are with the true redshifts.\n",
        "Ideally, it would be a uniform distribution, meaning N% of galaxies have their true redshift within the Nth percentile of their estimated photo-$z$ posterior PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCWuAkGWk2ik"
      },
      "outputs": [],
      "source": [
        "pit_values = cdf_coverage.cdf_coverage(cde_test, y_grid, y_test)\n",
        "pit_values_bias = cdf_coverage.cdf_coverage(cde_test_bias, y_grid_bias, y_test)\n",
        "\n",
        "plt.hist(pit_values, alpha=0.5, bins=100, label='representative', density=True)\n",
        "plt.hist(pit_values_bias, alpha=0.5, bins=100, label='biased', density=True)\n",
        "# plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "plt.xlabel('PIT')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKxtSTGtlrbq"
      },
      "source": [
        "The Highest Predictive Density (HPD) \n",
        "\\begin{equation}\n",
        "HPD = \\int_{z': p(z' | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi) \\geq p(z | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)} p(z' | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi) dz\n",
        "\\end{equation}\n",
        "is like the area of the PDF where it exceeds a given value.\n",
        "Over a population, it would ideally be flat, like the PIT.\n",
        "[A talk by David Zhao (CMU)](https://drive.google.com/file/d/1uvPtK_RcTUHEwt0ZYld41VKEPHnehWbN/view) has a lovely visualization of the HPD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84WUFQ8rfVtZ"
      },
      "outputs": [],
      "source": [
        "hpd_cov = hpd_coverage.hpd_coverage(cde_test, y_grid, y_test)\n",
        "hpd_cov_bias = hpd_coverage.hpd_coverage(cde_test_bias, y_grid_bias, y_test)\n",
        "\n",
        "plt.hist(hpd_cov, alpha=0.5, bins=100, label='representative', density=True)\n",
        "plt.hist(hpd_cov_bias, alpha=0.5, bins=100, label='biased', density=True)\n",
        "# plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "plt.xlabel('HPD')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgkvMM8enL48"
      },
      "source": [
        "The CDE loss \n",
        "\\begin{equation}\n",
        "\\hat{L} = \\frac{1}{K} \\sum_{i=1}^{K} \\int \\left(p(z | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)\\right)^{2} dz - \\frac{2}{K} \\sum_{i=1}^{K} p(z_{i} | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)\n",
        "\\end{equation}\n",
        "approximates the true posterior from the estimated posterior evaluated at the true redshift.\n",
        "It's explained quite well in [a talk by Nic Dalmasso (CMU)](https://www.dropbox.com/s/2r4tl4qv0iyqo9b/STAMPS_LSST_CDE_Tools_Presentation.pdf?dl=0).\n",
        "A lower value indicates a better estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQq6sv7JV4bH"
      },
      "outputs": [],
      "source": [
        "print(cde_loss.cde_loss(cde_test, y_grid, y_test))\n",
        "print(cde_loss.cde_loss(cde_test_bias, y_grid_bias, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenge 3f\n",
        "\n",
        "The implementations of the PIT, HPD, and CDE Loss above are evaluated on just a subset of the possible combinations of priors (training sets), test set data, and estimator hyperparameters.\n",
        "Evaluate them on the test cases you considered and compare them to the point estimate metrics.\n",
        "Do they tell you the same thing or something different?"
      ],
      "metadata": {
        "id": "y6koX3_FSzf9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ECou4SgJiQXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbTHU-y6xNKX"
      },
      "source": [
        "### Comparison of estimated and true photo-$z$ posterior PDFs\n",
        "\n",
        "Of course what we really want to know is how well the estimator captures the uncertainty inherent to the photometric data, i.e. how well it approximates the true posterior PDF!\n",
        "(Although we also want to know how that uncertainty quantification impacts \n",
        "\n",
        "`qp` [(Malz, et al 2018)](https://arxiv.org/abs/1806.00014) is a package for manipulating univariate PDFs under many parameterizations and includes a few comparison metrics.\n",
        "We'll use [the new version of qp](https://github.com/LSSTDESC/qp) for the sake of speed but evaluate metrics using simplified functions ported from [the old version](https://github.com/aimalz/qp) for robustness (because the new qp still has some bugs in checking the normalization of PDFs, oops).\n",
        "\n",
        "_This content is adapted from the [qp demo](https://github.com/LSSTDESC/qp/blob/master/docs/notebooks/demo.ipynb), written by Alex Malz (CMU), Phil Marshall (SLAC), and Eric Charles (SLAC), and [qp metrics demo](https://github.com/LSSTDESC/qp/blob/master/docs/notebooks/kld.ipynb), written by Alex Malz (CMU) and Phil Marshall (SLAC)._"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to get both the true posteriors and the approximations evaluated on the same grid of redshifts.\n",
        "Note that some metrics in this category compare approximate and true PDFs as if they are arbitrary functions, whereas others rely on the fact that they are normalized with $p(z | \\vec{d}) \\geq 0$ and $\\int p(z | \\vec{d}) dz = 1$."
      ],
      "metadata": {
        "id": "iQk_p13Fk0cN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WARGQ6Gh-k_"
      },
      "outputs": [],
      "source": [
        "# P = qp.Ensemble(qp.interp, data=dict(xvals=grid.reshape(grid.shape[0]), yvals=posteriors_test))\n",
        "Q = qp.Ensemble(qp.interp, data=dict(xvals=y_grid.reshape(y_grid.shape[0]), yvals=cde_test))\n",
        "Q_bias = qp.Ensemble(qp.interp, data=dict(xvals=y_grid.reshape(y_grid_bias.shape[0]), yvals=cde_test_bias))\n",
        "grid, approx_pdf_on_grid = Q.gridded(grid)\n",
        "grid, approx_pdf_on_grid_bias = Q_bias.gridded(grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR96wyRGPerk"
      },
      "source": [
        "The root-mean-square-error (RMSE) is a symmetric measure commonly used to compare any 1D functions.\n",
        "Similarly, a lower value corresponds to a more closely approximating posterior PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrtkIQZXwlMZ"
      },
      "outputs": [],
      "source": [
        "RMSEs = np.array([qp.metrics.quick_rmse(p, q, N=granularity) for p, q in zip(posteriors_test, approx_pdf_on_grid)])\n",
        "RMSEs_bias = np.array([qp.metrics.quick_rmse(p, q, N=granularity) for p, q in zip(posteriors_test, approx_pdf_on_grid_bias)])\n",
        "\n",
        "plt.hist(np.log(RMSEs), alpha=0.5, bins=100, label='representative', density=True)\n",
        "plt.hist(np.log(RMSEs_bias), alpha=0.5, bins=100, label='biased', density=True)\n",
        "plt.xlabel('RMSE')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioNxxbaDoDEm"
      },
      "source": [
        "The Kullback Leibler Divergence (KLD)\n",
        "\\begin{equation}\n",
        "KLD = \\int_{-\\infty}^{\\infty} p(z | \\vec{d}) \\log\\left[\\frac{p(z | \\vec{d}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)}{p(z | \\vec{d})}\\right] dz\n",
        "\\end{equation}\n",
        "is a directional measure of how much information is lost by using the estimated $p(z | \\vec{d}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)$ instead of the true $p(z | \\vec{d})$.\n",
        "We want the KLD for each galaxy to be low."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-J7YdTtLKPf"
      },
      "outputs": [],
      "source": [
        "KLDs = np.array([qp.metrics.quick_kld(p, q, dx=np.mean(grid[1:] - grid[:-1])) for p, q in zip(posteriors_test, approx_pdf_on_grid)])\n",
        "KLDs_bias = np.array([qp.metrics.quick_kld(p, q, dx=np.mean(grid[1:] - grid[:-1])) for p, q in zip(posteriors_test, approx_pdf_on_grid_bias)])\n",
        "\n",
        "plt.hist(np.log(KLDs), alpha=0.5, bins=100, label='representative', density=True)\n",
        "plt.hist(np.log(KLDs_bias), alpha=0.5, bins=100, label='biased', density=True)\n",
        "plt.xlabel('KLD')\n",
        "plt.legend()\n",
        "plt.semilogy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The appendix of [Malz+ (2018)](https://arxiv.org/abs/1806.00014) builds some intuition for the KLD in terms of the more familiar RMSE."
      ],
      "metadata": {
        "id": "TuivxyEMq5jE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenge 3g\n",
        "\n",
        "Try out these metrics on the various estimator outputs on the different data sets, and interpret the results.\n",
        "Why do different situations result in better or worse metric values?\n",
        "Do the metrics have different sensitivity to each type of imperfection?"
      ],
      "metadata": {
        "id": "jgjXqaUSrcE0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nBspqcrPrwZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTwhvAxUxVnf"
      },
      "source": [
        "### Challenge 3h\n",
        "\n",
        "Apply and visualize the local metrics of [Zhao, Dalmasso, Izbicki & Lee, 2021](https://arxiv.org/abs/2102.10473), or any other metrics not included in this demo; the [cde-diagnostics tutorial](https://github.com/zhao-david/CDE-diagnostics/blob/main/tutorial/tutorial-cde-diagnostics.ipynb), written by David Zhao (CMU), may be a good starting point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIwztiqPpgMU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "DSFP (Python 3)",
      "language": "python",
      "name": "dsfp_3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}